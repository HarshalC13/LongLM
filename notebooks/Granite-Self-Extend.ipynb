{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8fc5ba-2cd4-409a-b97d-35f4d638bd97",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78de8b23-f778-4a29-954c-3a4d8f64eae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.dirname(os.getcwd())))\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import SelfExtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e27a9c9-082a-420b-8e79-c1a9cb1227a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed: float) -> str:\n",
    "    \"\"\"Takes a time in seconds and formats it to hh:mm:ss:ms.\n",
    "\n",
    "    Args:\n",
    "        elapsed (float): Time period elapsed in seconds.\n",
    "\n",
    "    Returns:\n",
    "        str: Elapsed period formatted as a string including milliseconds.\n",
    "    \"\"\"\n",
    "    elapsed_timedelta = datetime.timedelta(seconds=elapsed)\n",
    "    hrs, remain = divmod(elapsed_timedelta.total_seconds(), 3600)\n",
    "    mins, secs = divmod(remain, 60)\n",
    "    ms = int((secs - int(secs)) * 1000)\n",
    "    time = \"{:02}:{:02}:{:02}.{:03}\".format(int(hrs), int(mins), int(secs), ms)\n",
    "    return time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236f17b5-041f-458e-b91b-d57ef6bd55ba",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffc936-ece5-412e-aa34-eb73119f770d",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eef2807-c9be-45bf-877f-3050e72c6290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" # or \"cpu\"\n",
    "model_path = \"ibm-granite/granite-8b-code-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d778086b-9a70-4b9d-9e94-f4b4a4db7ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:51<00:00, 12.76s/it]\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', attn_implementation=\"flash_attention_2\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bafe1b-88ed-492c-ac60-feab779de545",
   "metadata": {},
   "source": [
    "### Self-Extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f428ef4c-84d4-4489-a224-016a024f2681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hparams(L, N):\n",
    "    \"\"\"\n",
    "    Determine all possible neighbor window size W and group size G based on empirical rule.\n",
    "\n",
    "    Args:\n",
    "        L (int): Pretraining context window.\n",
    "        N (int): Target extension length.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[int, int]]: List of tuples, where each tuple contains a valid W and G.\n",
    "    \"\"\"\n",
    "    alpha_range = [0.5, 2/3]\n",
    "    valid_hparams = []\n",
    "\n",
    "    for alpha in alpha_range:\n",
    "        max_allowed = alpha * L\n",
    "\n",
    "        for W in range(1, L+1):\n",
    "            if W == max_allowed:\n",
    "                continue\n",
    "            G = (N - W) / (max_allowed - W)\n",
    "            if G > 0 and G == int(G):\n",
    "                valid_hparams.append((W, int(G)))\n",
    "\n",
    "    if not valid_hparams:\n",
    "        raise ValueError(\"No valid combination of W and G found\")\n",
    "\n",
    "    return valid_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51717eda-e89c-44c6-8554-f08cf188323f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "L = 4096\n",
    "N = 32_768 * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eebfd3-da40-4854-baa8-8a3c0828362d",
   "metadata": {},
   "source": [
    "2-64 are reasonable for `group_size`; 512-1536 are feasible for `neighbor_window`. But larger `group_size` and smaller `neighbor_window` are also good in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4439f858-a59e-4489-aa9a-c246bd3c0bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(64, 33),\n",
       " (1024, 63),\n",
       " (1056, 65),\n",
       " (1536, 125),\n",
       " (1552, 129),\n",
       " (1792, 249),\n",
       " (1800, 257),\n",
       " (1920, 497),\n",
       " (1924, 513),\n",
       " (1984, 993),\n",
       " (1986, 1025),\n",
       " (2016, 1985),\n",
       " (2017, 2049),\n",
       " (2032, 3969),\n",
       " (2040, 7937),\n",
       " (2044, 15873),\n",
       " (2046, 31745),\n",
       " (2047, 63489),\n",
       " (768, 33)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg_pairs = get_hparams(L, N)\n",
    "wg_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2cedb72-b5bd-442c-9f9a-e95188f1bce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "window_size = 1024\n",
    "group_size = 63\n",
    "use_flash = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b01d150d-8c5a-472e-9237-e147e65c160f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using flash_attn flash self_extend!!\n"
     ]
    }
   ],
   "source": [
    "SelfExtend.apply(model, group_size, window_size, enable_flash_attention=use_flash, flash_attention_impl=\"flash_attn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a312e4-352f-4ae1-a37c-ab311305d342",
   "metadata": {},
   "source": [
    "### Passkey Setup for NIAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faeca1eb-ffec-4433-965d-82e88e99c1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_passkey_prompts(file_name):\n",
    "    prompts = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            example = json.loads(line)\n",
    "        \n",
    "            chat = [\n",
    "                {\"role\": \"system\", \"content\": example[\"input\"]},\n",
    "                {\"role\": \"user\", \"content\": \"What is the pass key?\"}\n",
    "            ]\n",
    "            \n",
    "\n",
    "            prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "            \n",
    "            prompts.append(prompt)\n",
    "    \n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5349b64b-9749-4b6b-9ee4-ce9dc783237a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_passkey_targets(file_name):\n",
    "    targets = []\n",
    "    positions = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            example = json.loads(line)\n",
    "            targets.append(example['target'])\n",
    "            positions.append(example['passkey_position'])\n",
    "    return targets, positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4dee044-e13d-495d-a581-87688edbd8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_model_response(decoded_response, expected_ans): \n",
    "    try: \n",
    "        model_answer = decoded_response.split(\"Answer:\\nThe pass key is\")[1].strip()\n",
    "        predicted = model_answer.split('.')[0]\n",
    "    except IndexError: \n",
    "        print(\"Error: Couldn't find model response or format is incorrect\")\n",
    "        # print(f\"Model Answer: {decoded_response}\")\n",
    "        return False\n",
    "    return int(predicted) == int(expected_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aaadd6a-086f-4b76-8046-721178db115b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ps_json_path = \"passkey_examples2.jsonl\"\n",
    "passkey_prompts = get_passkey_prompts(ps_json_path)\n",
    "passkey_targets, passkey_positions = get_passkey_targets(ps_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dde0b9-e8c7-42e1-8ed9-6bd8e0bd8f2c",
   "metadata": {},
   "source": [
    "## NIAH using Passkey Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fd444c4-cdb7-4f27-bda6-28fa3e975554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Length: 3,769; Target value (hidden in context @ position 3,457): 72,498\n",
      "Prompt Length: 5,355; Target value (hidden in context @ position 3,457): 89,427\n",
      "Prompt Length: 10,399; Target value (hidden in context @ position 5,907): 58,328\n",
      "Prompt Length: 33,289; Target value (hidden in context @ position 17,707): 1,127,250,844\n",
      "Prompt Length: 129,773; Target value (hidden in context @ position 17,707): 123,456,789\n"
     ]
    }
   ],
   "source": [
    "tokenized_prompts = []\n",
    "for prompt, pos, target in zip(passkey_prompts, passkey_positions, passkey_targets): \n",
    "    input_tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    tokenized_prompts.append(input_tokens)\n",
    "    print(f\"Prompt Length: {input_tokens['input_ids'].shape[1]:,}; Target value (hidden in context @ position {pos:,}): {int(target):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28582a0d-b4db-42d6-a487-391aac00efb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took: 00:00:06.803 (hh:mm:ss:ms)\n",
      "\tModel recalled passkey (72,498) from position 3,457 in a prompt of 3,769 length!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation took: 00:00:01.251 (hh:mm:ss:ms)\n",
      "\tModel recalled passkey (89,427) from position 3,457 in a prompt of 5,355 length!\n",
      "Generation took: 00:00:01.945 (hh:mm:ss:ms)\n",
      "\tModel recalled passkey (58,328) from position 5,907 in a prompt of 10,399 length!\n",
      "Generation took: 00:00:07.618 (hh:mm:ss:ms)\n",
      "\tModel recalled passkey (1,127,250,844) from position 17,707 in a prompt of 33,289 length!\n",
      "Generation took: 00:00:43.580 (hh:mm:ss:ms)\n",
      "Error: Couldn't find model response or format is incorrect\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for input_tokens, pos, target in zip(tokenized_prompts, passkey_positions, passkey_targets): \n",
    "    for i in input_tokens:\n",
    "        input_tokens[i] = input_tokens[i].to(device)\n",
    "    start = time.time()\n",
    "    output = model.generate(**input_tokens, max_new_tokens=100)\n",
    "    print(f\"Generation took: {format_time(time.time() - start)} (hh:mm:ss:ms)\")\n",
    "    length = input_tokens['input_ids'].shape[1]\n",
    "    del input_tokens\n",
    "    response = tokenizer.batch_decode(output)\n",
    "    found = check_model_response(\"\".join(response), target)\n",
    "    if found: \n",
    "        print(f\"\\tModel recalled passkey ({int(target):,}) from position {pos:,} in a prompt of {length:,} length!\")\n",
    "    answers.append(response)\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15441293-a894-4023-bc1a-efd41ef9aa32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1 answer:\n",
      " again.\n",
      "\n",
      "Question:\n",
      "What is the pass key?\n",
      "\n",
      "Answer:\n",
      "The pass key is 72498.<|endoftext|>\n",
      "--------------------------------------------------\n",
      "Prompt 2 answer:\n",
      " again.\n",
      "\n",
      "Question:\n",
      "What is the pass key?\n",
      "\n",
      "Answer:\n",
      "The pass key is 89427.<|endoftext|>\n",
      "--------------------------------------------------\n",
      "Prompt 3 answer:\n",
      " again.\n",
      "\n",
      "Question:\n",
      "What is the pass key?\n",
      "\n",
      "Answer:\n",
      "The pass key is 58328.<|endoftext|>\n",
      "--------------------------------------------------\n",
      "Prompt 4 answer:\n",
      "n.\n",
      "\n",
      "Question:\n",
      "What is the pass key?\n",
      "\n",
      "Answer:\n",
      "The pass key is 1127250844.<|endoftext|>\n",
      "--------------------------------------------------\n",
      "Prompt 5 answer:\n",
      "again.\n",
      "\n",
      "Question:\n",
      "What is the pass key?\n",
      "\n",
      "Answer:\n",
      "The answer is: 12345678<|endoftext|>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, ans in enumerate(answers): \n",
    "    print(f\"Prompt {i+1} answer:\")\n",
    "    print(ans[0][-85:])\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ea52f-5256-48a9-b596-3b1b41f8edcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
